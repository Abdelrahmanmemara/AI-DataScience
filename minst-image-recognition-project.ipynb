{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3004,"databundleVersionId":861823,"sourceType":"competition"}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom sklearn.datasets import load_digits\nfrom torch.utils.data import DataLoader, Dataset\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-23T17:41:41.319794Z","iopub.execute_input":"2024-07-23T17:41:41.321115Z","iopub.status.idle":"2024-07-23T17:41:41.332808Z","shell.execute_reply.started":"2024-07-23T17:41:41.321055Z","shell.execute_reply":"2024-07-23T17:41:41.331321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading the Dataset\n\n- In this part we load the digits dataset that we are going to use.\n\n- We seperate the data into 3 different categories:- training, validation, and testing.","metadata":{}},{"cell_type":"code","source":"train_input = np.array(pd.read_csv(\"/kaggle/input/digit-recognizer/train.csv\"))[0:33000]\n\nvalidation_input = np.array(pd.read_csv(\"/kaggle/input/digit-recognizer/train.csv\"))[33000:]\n\ntest_input = np.array(pd.read_csv(\"/kaggle/input/digit-recognizer/test.csv\"))","metadata":{"execution":{"iopub.status.busy":"2024-07-23T17:40:07.443737Z","iopub.execute_input":"2024-07-23T17:40:07.444283Z","iopub.status.idle":"2024-07-23T17:40:15.720917Z","shell.execute_reply.started":"2024-07-23T17:40:07.444253Z","shell.execute_reply":"2024-07-23T17:40:15.719709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_input.shape","metadata":{"execution":{"iopub.status.busy":"2024-07-21T17:42:37.936767Z","iopub.execute_input":"2024-07-21T17:42:37.937136Z","iopub.status.idle":"2024-07-21T17:42:37.945670Z","shell.execute_reply.started":"2024-07-21T17:42:37.937108Z","shell.execute_reply":"2024-07-21T17:42:37.944364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"validation_input.shape","metadata":{"execution":{"iopub.status.busy":"2024-07-21T17:42:41.802529Z","iopub.execute_input":"2024-07-21T17:42:41.802907Z","iopub.status.idle":"2024-07-21T17:42:41.810176Z","shell.execute_reply.started":"2024-07-21T17:42:41.802879Z","shell.execute_reply":"2024-07-21T17:42:41.809003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_input.shape","metadata":{"execution":{"iopub.status.busy":"2024-07-21T17:42:45.548083Z","iopub.execute_input":"2024-07-21T17:42:45.549135Z","iopub.status.idle":"2024-07-21T17:42:45.556248Z","shell.execute_reply.started":"2024-07-21T17:42:45.549098Z","shell.execute_reply":"2024-07-21T17:42:45.554715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This is a class for the dataset of small (8px x 8px) digits.\n# Please try to understand in details how it works!\nclass Digits(Dataset):\n    \"\"\"Scikit-Learn Digits dataset.\"\"\"\n\n    def __init__(self, mode=\"train\", transforms=None):\n        self.data = []\n        self.targets = []\n        if mode == \"train\":\n            for i in range(train_input.shape[0]):\n                self.data.append(np.array(train_input[i][1:]).astype(np.float32))\n                self.targets.append(np.array(train_input[i][0]).astype(np.float32))\n            self.data = torch.tensor(np.array(self.data), requires_grad=True)\n            self.targets = torch.tensor(np.array(self.targets), requires_grad=True)\n                \n        elif mode == \"val\":\n            for i in range(validation_input.shape[0]):\n                self.data.append(np.array(validation_input[i][1:]).astype(np.float32))\n                self.targets.append(np.array(validation_input[i][0]).astype(np.float32))\n            self.data = torch.tensor(np.array(self.data), requires_grad=True)\n            self.targets = torch.tensor(np.array(self.targets), requires_grad=True)\n            \n        else:\n            for i in range(test_input.shape[0]):\n                self.data.append(np.array(test_input[i]).astype(np.float32))\n                self.targets.append(np.array(test_input[i][0]).astype(np.float32))\n            self.data = torch.tensor(np.array(self.data), requires_grad=True)\n            self.targets = torch.tensor(np.array(self.targets), requires_grad=True)\n            \n\n        self.transforms = transforms\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        sample_x = self.data[idx]\n        sample_y = self.targets[idx]\n        if self.transforms:\n            sample_x = self.transforms(sample_x)\n        return (sample_x, sample_y)","metadata":{"execution":{"iopub.status.busy":"2024-07-23T23:13:40.258543Z","iopub.execute_input":"2024-07-23T23:13:40.258928Z","iopub.status.idle":"2024-07-23T23:13:40.273288Z","shell.execute_reply.started":"2024-07-23T23:13:40.258899Z","shell.execute_reply":"2024-07-23T23:13:40.272158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Here, we plot some images (8px x 8px).\ndigits = load_digits()\nx = digits.data[:16].astype(np.float32)\n\nfig_data, axs = plt.subplots(4, 4, figsize=(4, 4))\nfig_data.tight_layout()\n\nfor i in range(4):\n    for j in range(4):\n        img = np.reshape(x[4 * i + j], (8, 8))\n        axs[i, j].imshow(img, cmap=\"gray\")\n        axs[i, j].axis(\"off\")","metadata":{"execution":{"iopub.status.busy":"2024-07-18T12:42:39.552401Z","iopub.execute_input":"2024-07-18T12:42:39.553562Z","iopub.status.idle":"2024-07-18T12:42:40.590671Z","shell.execute_reply.started":"2024-07-18T12:42:39.553518Z","shell.execute_reply":"2024-07-18T12:42:40.589183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Neural Network\n\n- We will use Convolutional Neural Network architecture (CNN) to build our Neural Network. \n","metadata":{}},{"cell_type":"code","source":"\nclass ConvolutionalNN(nn.Module):\n    def __init__(self):\n        super(ConvolutionalNN, self).__init__()\n        \n        # The Loss function \n        self.nll = nn.NLLLoss()\n        \n        # The Neural Network Structure\n        self.Conv1 = nn.Conv2d(1,10,3)\n        self.Conv2 = nn.Conv2d(10,20,5)\n        \n        self.BatchNorm1 = nn.BatchNorm2d(10)\n        self.BatchNorm2 = nn.BatchNorm2d(20)\n\n        self.flatten_dim = 20 * 4 * 4  # This will be calculated dynamically based on input size\n        \n        self.Linear1 = nn.Linear(320, 1060)\n        self.Linear2 = nn.Linear(1060,500)\n        self.Linear3 = nn.Linear(500,100)\n        self.Linear4 = nn.Linear(100,50)\n        self.Linear5 = nn.Linear(50,10)\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n\n    \n    def forward(self,x):\n        \n        # The full forward pass of the Neural Network\n            \n        data = x.view(x.shape[0],1,28,28)\n        self.Layer1 = F.relu(self.BatchNorm1(self.Conv1(data)))\n        self.Layer2 = self.pool(self.Layer1)\n        self.Layer0 = self.BatchNorm2(self.Conv2(self.Layer2))\n        self.Layer3 = F.relu(self.Layer0)\n        self.Layer4 = self.pool(self.Layer3)\n        self.Layer4 = self.Layer4.view(self.Layer4.size(0), -1)\n        self.Layer5 = self.Linear1(self.Layer4)\n        self.Layer6 = F.leaky_relu(self.Layer5)\n        self.Layer7 = F.leaky_relu(self.Linear2(self.Layer6))\n        self.Layer8 = F.relu(self.Linear3(self.Layer7))\n        self.Layer9 = F.relu(self.Linear4(self.Layer8))\n        self.Layer10 = F.sigmoid(self.Layer9)\n        self.Layer11 = self.Linear5(self.Layer10)\n        output = F.log_softmax(self.Layer11, dim=1)\n        \n        return output\n        \n        \n    def classify(self,x):\n        \n        classified = torch.max(x, 1)\n        \n        return classified \n    def loss(self,x,y):\n\n        loss = self.nll(x, y.long())\n        return loss.mean()\n","metadata":{"execution":{"iopub.status.busy":"2024-07-23T23:03:22.830302Z","iopub.execute_input":"2024-07-23T23:03:22.831247Z","iopub.status.idle":"2024-07-23T23:03:22.844882Z","shell.execute_reply.started":"2024-07-23T23:03:22.831175Z","shell.execute_reply":"2024-07-23T23:03:22.843449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training Procedure","metadata":{}},{"cell_type":"markdown","source":"- Loading the training, validation, and training data","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import TensorDataset, DataLoader\n\n\ntrain = DataLoader(Digits(mode= \"train\"), batch_size= 64, shuffle = True)\nvalidation = DataLoader(Digits(mode=\"val\"), batch_size=64, shuffle = True)\ntest =  DataLoader(Digits(mode=\"test\"), batch_size=28000, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-07-23T23:22:53.219972Z","iopub.execute_input":"2024-07-23T23:22:53.220570Z","iopub.status.idle":"2024-07-23T23:22:54.010066Z","shell.execute_reply.started":"2024-07-23T23:22:53.220531Z","shell.execute_reply":"2024-07-23T23:22:54.008774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.optim as optim\n\nnum_epochs = 10\nloss_validation_list = []\nprecision_validation = []\nrecall_validation = []\nmodel = ConvolutionalNN()\ntry:\n    model.load_state_dict(torch.load('/kaggle/working/model.pth'))\n    print(\"Model loaded from model.pth\")\nexcept FileNotFoundError:\n    print(\"No saved model found, training from scratch\")\n# We have to initialize the optimizer\noptimizer = optim.SGD(model.parameters(), lr=0.001, momentum= 0.95)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-23T17:42:05.016572Z","iopub.execute_input":"2024-07-23T17:42:05.016940Z","iopub.status.idle":"2024-07-23T17:42:05.036584Z","shell.execute_reply.started":"2024-07-23T17:42:05.016913Z","shell.execute_reply":"2024-07-23T17:42:05.035251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss_training = 0.0\nloss_training_list = []\n\nfor epoch in range(num_epochs):\n    loss_training = 0.0\n    model.train()\n    for number, (features, targets) in enumerate(train):\n        optimizer.zero_grad()\n        forward = model.forward(features)\n        output = model.loss(forward,targets)\n        output.backward()\n        optimizer.step()\n\n        loss_training += output\n\n        \n        \n\n    loss_training = loss_training/len(train)\n    loss_training_list.append(loss_training)\n    print(f\"The training loss for the epoch {epoch+1} : {loss_training}\")\n    \n    print(f\"The validation statistics for epoch {epoch+1}: \")\n    validation_loss = evaluate(validation,model,average=\"macro\")\n    loss_validation_list.append(validation_loss[\"loss\"])\n    precision_validation.append(validation_loss[\"precision\"])\n    recall_validation.append(validation_loss[\"recall\"])\n    print(f\"The validation loss for the epoch {epoch+1} : {validation_loss}\")\n\ntorch.save(model.state_dict(), \"model.pth\")\n","metadata":{"execution":{"iopub.status.busy":"2024-07-23T17:42:07.419927Z","iopub.execute_input":"2024-07-23T17:42:07.420341Z","iopub.status.idle":"2024-07-23T22:36:31.098137Z","shell.execute_reply.started":"2024-07-23T17:42:07.420303Z","shell.execute_reply":"2024-07-23T22:36:31.095313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(model.state_dict(), \"model.pth\")","metadata":{"execution":{"iopub.status.busy":"2024-07-23T22:48:31.363731Z","iopub.execute_input":"2024-07-23T22:48:31.364250Z","iopub.status.idle":"2024-07-23T22:48:31.382748Z","shell.execute_reply.started":"2024-07-23T22:48:31.364201Z","shell.execute_reply":"2024-07-23T22:48:31.381514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_list = [loss.detach().item() for loss in loss_training_list ]\nx= [i for i in range(1,11)]\nplt.plot(x,training_list, label= \"Training\")\nplt.plot(x,loss_validation_list, label= \"Validation\")\nplt.title(\"Validation loss vs Training loss\")\nplt.xlabel(\"Number of Epochs\")\nplt.ylabel(\"NLL loss\")\nplt.legend()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-07-23T22:53:28.740968Z","iopub.execute_input":"2024-07-23T22:53:28.741413Z","iopub.status.idle":"2024-07-23T22:53:29.010624Z","shell.execute_reply.started":"2024-07-23T22:53:28.741362Z","shell.execute_reply":"2024-07-23T22:53:29.009124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Function to evaluate the model\n\n- This will be used in the training loop to evaluate the model on the validation data.\n\n- Will also be used in the end to evaluate the model on the test data.\n","metadata":{}},{"cell_type":"code","source":"from torchmetrics import Precision, Recall","metadata":{"execution":{"iopub.status.busy":"2024-07-23T17:39:44.930434Z","iopub.execute_input":"2024-07-23T17:39:44.931124Z","iopub.status.idle":"2024-07-23T17:39:52.137094Z","shell.execute_reply.started":"2024-07-23T17:39:44.931088Z","shell.execute_reply":"2024-07-23T17:39:52.135951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate(data, model, average=\"None\"):\n    \n    loss = 0.0\n    precision_metric = Precision(task=\"multiclass\", num_classes=10, average=average)\n    recall_metric = Recall(task=\"multiclass\", num_classes=10, average=average)\n    \n    \n    model.eval()\n    \n    with torch.no_grad():\n        \n        for idx_batch, (batch,targets) in enumerate(data):\n            forward = model.forward(batch)\n            loss += model.loss(forward,targets)\n            precision_metric(forward,targets)\n            recall_metric(forward,targets)\n            \n            \n    loss = loss/len(data)\n    precision = precision_metric.compute()\n    recall = recall_metric.compute()\n    \n    print(f\"The loss is: {loss}\")\n    print(f\"The precision is {precision}\")\n    print(f\"The recall is {recall}\")\n    \n    \n    return {\"loss\":loss, \"precision\": precision, \"recall\":recall}","metadata":{"execution":{"iopub.status.busy":"2024-07-23T22:54:15.963206Z","iopub.execute_input":"2024-07-23T22:54:15.963654Z","iopub.status.idle":"2024-07-23T22:54:15.972086Z","shell.execute_reply.started":"2024-07-23T22:54:15.963620Z","shell.execute_reply":"2024-07-23T22:54:15.970704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"for features,_ in test:\n    prediction = model.forward(features)\n    test_predictions = model.classify(prediction)[1]\n    ","metadata":{"execution":{"iopub.status.busy":"2024-07-23T23:23:03.039039Z","iopub.execute_input":"2024-07-23T23:23:03.039465Z","iopub.status.idle":"2024-07-23T23:23:08.537703Z","shell.execute_reply.started":"2024-07-23T23:23:03.039429Z","shell.execute_reply":"2024-07-23T23:23:08.536194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_predictions[0:1].item()","metadata":{"execution":{"iopub.status.busy":"2024-07-23T23:23:48.682171Z","iopub.execute_input":"2024-07-23T23:23:48.683273Z","iopub.status.idle":"2024-07-23T23:23:48.691945Z","shell.execute_reply.started":"2024-07-23T23:23:48.683233Z","shell.execute_reply":"2024-07-23T23:23:48.690453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = np.array([pred.item() for pred in test_predictions])\n\npredictions[0:5]","metadata":{"execution":{"iopub.status.busy":"2024-07-23T23:25:18.278249Z","iopub.execute_input":"2024-07-23T23:25:18.279156Z","iopub.status.idle":"2024-07-23T23:25:18.418352Z","shell.execute_reply.started":"2024-07-23T23:25:18.279118Z","shell.execute_reply":"2024-07-23T23:25:18.417171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ImageIds = np.arange(1,28001)","metadata":{"execution":{"iopub.status.busy":"2024-07-23T23:26:48.776523Z","iopub.execute_input":"2024-07-23T23:26:48.777174Z","iopub.status.idle":"2024-07-23T23:26:48.786575Z","shell.execute_reply.started":"2024-07-23T23:26:48.777132Z","shell.execute_reply":"2024-07-23T23:26:48.785283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Dataframe = {\n    \"ImageId\": ImageIds,\n    \"Label\": predictions\n}\n\nDf = pd.DataFrame(Dataframe)\n\nDf.set_index(\"ImageId\", inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-07-23T23:29:38.171324Z","iopub.execute_input":"2024-07-23T23:29:38.171784Z","iopub.status.idle":"2024-07-23T23:29:38.180322Z","shell.execute_reply.started":"2024-07-23T23:29:38.171752Z","shell.execute_reply":"2024-07-23T23:29:38.178952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Df.head()","metadata":{"execution":{"iopub.status.busy":"2024-07-23T23:29:39.488750Z","iopub.execute_input":"2024-07-23T23:29:39.489173Z","iopub.status.idle":"2024-07-23T23:29:39.502873Z","shell.execute_reply.started":"2024-07-23T23:29:39.489141Z","shell.execute_reply":"2024-07-23T23:29:39.500493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Df.to_csv(\"Submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-07-23T23:29:43.599500Z","iopub.execute_input":"2024-07-23T23:29:43.599929Z","iopub.status.idle":"2024-07-23T23:29:43.628136Z","shell.execute_reply.started":"2024-07-23T23:29:43.599899Z","shell.execute_reply":"2024-07-23T23:29:43.626957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}